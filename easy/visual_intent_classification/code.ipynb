{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Описание задачи\n",
    "Наши студенты задают множество вопросов в общем чате на совершенно различные темы: от общих тем и вопросов оплаты курсов, до трудоустройства и даже вопросов, не относящихся к области деятельности платформы. Для ускорения обработки запросов и снижения нагрузки на поддержку было принято решение создать Telegram-бота для автоматизации ответов и эскалации сложных запросов к живым операторам.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21ad4367a96be5e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CLIP: архитектура модели\n",
    "В 2021 году появилась мультимодальная модель CLIP (Contrastive Language–Image Pre-training; не путать... ох, ни с чем не путать) от OpenAI. Модель обучается на большом объёме данных, сочетающих текст и изображения, и способна понимать связь между ними."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74700f594df12ac2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Contrastive Learning — это Metric Learning подход обучения, основанный на измерении расстояния между объектами в пространстве. Обучение происходит следующим образом: вектора похожих объектов \"приближаются\" друг к другу, а вектора разных \"раздвигаются\", т.е. мы учим модель проводить более чёткую (контрастную) линию между похожими и непохожими объектами - отсюда и название Contrastive Learning  \n",
    "\n",
    "Предобучение (pre-training) — это этап обучения модели машинного обучения, на котором модель инициализируется и обучается на большом объеме данных. Целью pre-training'a является получение моделью общих навыков, которые могут быть полезны для широкого круга задач. \n",
    "\n",
    "Image Encoder — модель машинного обучения, преобразующая изображения в эмбеддинги. В результате работы Image Encoder'a, изображение становится некой точкой в многомерном пространстве эмбеддингов.\n",
    "\n",
    "Zero-shot prediction — способность модели машинного обучения решать задачи, на которые она специально не была обучена. Например, классифицировать объекты между классами, которые не были в явном виде представлены в обучающей выборке."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332da69f000fee76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обучение CLIP происходит на батче из пар \"картинка-текст\". Каждая картинка проходит через Image Encoder, а каждый текст — через Text Encoder. В результате получаются пары векторных представлений, находящиеся в едином пространстве. \n",
    "\n",
    "Далее, вычисляется матрица всех возможных скалярных произведений пар этих векторов, которая в дальнейшем служит для определения степени сходства между каждой парой изображений и текстовых описаний.\n",
    "\n",
    "Для обучения модели CLIP используется метод максимизации косинусной близости (Cosine Similarity), которая нацелена на уменьшение угла между векторами правильных пар \"картинка-текст\" и увеличение между неправильными парами."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d40aac96d97d27d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Косинус угла достигает своего максимального значения (1), когда угол θ = 0 градусов (вектора сонаправлены), и минимального значения (-1), когда угол θ = 180 градусов (вектора противоположно направлены).\n",
    "\n",
    "Таким образом, скалярные произведения, находящиеся на главной диагонали матрицы, будут максимизированы, поскольку они представляют собой скалярные произведения между правильными парами \"картинка-текст\". Все остальные скалярные произведения, которые соответствуют неправильным парам, будут минимизированы.\n",
    "\n",
    "Этот подход позволяет модели научиться точно связывать изображения и тексты, понимая их семантические соответствия.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d5ee64c4c739916"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use for zero-shot prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "906655e656191588"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Таким образом, чтобы классифицировать изображение, достаточно извлечь векторные представления для этого изображения, а также векторные представления для всех возможных категорий, которые могут быть предположительно классифицированы.\n",
    "\n",
    "Далее, моделью вычисляются скалярные произведения между эмбеддингом изображения и каждым из эмбеддингов текстовых меток. Эти значения служат мерами сходства, показывающими, насколько каждое из текстовых описаний соответствует изображению.\n",
    "\n",
    "Наконец, текстовая метка с наибольшим значением сходства выбирается как предсказанная категория для изображения. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78fd04778d39b052"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Преимущества CLIP\n",
    "1. \"Обучение без обучения\" (zero-shot prediction)\n",
    "CLIP демонстрирует выдающиеся способности к классификации объектов, которые не были представлены в тренировочном наборе данных. Это означает, что модель может распознавать и классифицировать новые, ранее невиданные объекты, без необходимости дополнительной разметки и обучения на этих конкретных данных. Таким образом, модель снижает необходимость в дорогостоящей и трудоемкой разметке данных.\n",
    "\n",
    "2. Мультимодальное понимание\n",
    "CLIP успешно решает широкий спектр задач в компьютерном зрении и NLP благодаря эффективному мультимодальному обучению, что позволяет ему работать с задачами, требующими смешения картинок и текста. Например, классифицировать визуальные запросы пользователей на основе их текстовых описаний (чем мы и займемся в этой задаче). Высокая точность и производительность модели делают её надежным выбором для критически важных приложений и сервисов.\n",
    "\n",
    "3. Гибкость и универсальность\n",
    "CLIP может использоваться для решения широкого спектра задач, включая классификацию изображений, генерацию описаний, поиск по изображениям и многое другое. Универсальность модели делает её полезной в различных приложениях и позволяет интегрировать её в различные системы и сервисы."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f56c92dc3f08820"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Недостатки CLIP\n",
    "1. Подверженность текстовым атрибутам\n",
    "Недостатком CLIP является подверженность текстовым атрибутам. Модель CLIP может быть сильно подвержена влиянию текстовой информации на изображении, иногда в ущерб визуальной информации. Это приводит к тому, что модель может идентифицировать объект не по его визуальным характеристикам, а по тексту, присутствующему на изображении.\n",
    "2. Качество данных\n",
    "CLIP обучался на больших объемах данных из интернета. Не все данные, найденные в интернете, являются качественными или точными, что может повлиять на производительность модели.\n",
    "\n",
    "3. Производительность в специфических задачах\n",
    "CLIP может не работать так же хорошо в узкоспециализированных областях или задачах, для которых у неё недостаточно данных во время обучения. Кроме того, модель может плохо работать с языками или диалектами, которые недостаточно хорошо представлены в обучающих данных.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a23bdb69ab57167"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание\n",
    "В этом задании мы будем использовать модель CLIP-ViT-tiny-large-patch14. Это уменьшенная версия модели CLIP, которая в качестве Image Encoder использует ViT-L/14. \n",
    "\n",
    "1. Создайте класс для подготовки изображения\n",
    "Напишите класс ImageProcessor, который будет отвечать за подготовку изображения для дальнейшей обработки.\n",
    "\n",
    "В классе должен быть метод prepare_image, который принимает путь к изображению, читает его, кодирует в base64 и возвращает подготовленное изображение."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f975af497dffc68"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T12:11:23.832390500Z",
     "start_time": "2024-10-31T12:11:23.822862400Z"
    }
   },
   "id": "647b08d0818aa15"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-31T12:11:24.173746500Z",
     "start_time": "2024-10-31T12:11:24.170233300Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing images.\n",
    "\n",
    "    Attributes:\n",
    "        image_path (str): The path to the image.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_path: str):\n",
    "        \"\"\"\n",
    "        Initialize an ImageProcessor instance.\n",
    "\n",
    "        Parameters:\n",
    "            image_path (str): The path to the image.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the image path is empty.\n",
    "        \"\"\"\n",
    "        if not image_path:\n",
    "            raise ValueError(\"Image path is empty.\" )\n",
    "        if not os.path.exists(image_path):\n",
    "            raise ValueError(\"Image file does not exist at the provided path.\")\n",
    "        self.image_path = image_path \n",
    "\n",
    "    def prepare_image(self) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Method to prepare the image for processing.\n",
    "\n",
    "        Returns:\n",
    "            Image.Image: The prepared image.\n",
    "        \"\"\"\n",
    "        # try:\n",
    "        #     # Open the image\n",
    "        #     with Image.open(self.image_path) as image:\n",
    "        #         buffered = io.BytesIO()\n",
    "        #         image.save(buffered, format=image.format) \n",
    "        #         image_bytes = buffered.getvalue()\n",
    "        # \n",
    "        #         encoded_image = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        # \n",
    "        #     return encoded_image\n",
    "        try:\n",
    "            # Open the image and return it directly\n",
    "            image = Image.open(self.image_path).convert(\"RGB\")\n",
    "            return image\n",
    "\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"Unable to process the image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Создайте класс для подготовки модели CLIP\n",
    "Напишите класс ClipModel, который будет отвечать за загрузку и подготовку модели CLIP для классификации.\n",
    "\n",
    "В классе должен быть метод classify, который принимает данные для классификации и возвращает распределение вероятностей в виде списка. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7316b792efd4396f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import torch\n",
    "class ClipModel:\n",
    "    \"\"\"\n",
    "    A class for working with a classification model.\n",
    "\n",
    "    Attributes:\n",
    "        model (CLIPModel): The CLIP model instance.\n",
    "        processor (CLIPProcessor): The CLIP processor instance.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a ClipModel instance.\n",
    "        \"\"\"\n",
    "        # Load model directly\n",
    "        self.model = AutoModelForZeroShotImageClassification.from_pretrained(\"yujiepan/clip-vit-tiny-random-patch14-336\")\n",
    "        self.processor = AutoProcessor.from_pretrained(\"yujiepan/clip-vit-tiny-random-patch14-336\")\n",
    "\n",
    "    def classify(self, image: Image.Image, intents: list) -> list:\n",
    "        \"\"\"\n",
    "        Method for classifying data.\n",
    "\n",
    "        Parameters:\n",
    "            image (Image.Image): The image to classify.\n",
    "            intents (list): The list of text categories.\n",
    "\n",
    "        Returns:\n",
    "            list: The classification result as probability distribution.\n",
    "        \"\"\"\n",
    "        inputs = self.processor(text=intents, images=image, return_tensors=\"pt\", padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Extract scores and normalize them to probabilities\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "        return probs[0].detach().numpy().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T12:11:24.994706400Z",
     "start_time": "2024-10-31T12:11:24.963181900Z"
    }
   },
   "id": "f939ff8d4aa2e870"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Создайте класс для классификации изображения\n",
    "Напишите класс ImageClassifier, который будет использовать ImageProcessor для подготовки изображения и ClipModel для его классификации.\n",
    "\n",
    "В классе должен быть метод classify_intent, который принимает путь к изображению, подготавливает его, отправляет его в модель для классификации и возвращает результат.\n",
    "\n",
    "Метод classify_intent должен возвращать один из трех тегов в виде строки: \"payment-problem\", \"course-problem\", \"profile-problem\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5e5b6a35a76db66"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class ImageClassifier:\n",
    "    \"\"\"\n",
    "    A class for classifying images.\n",
    "\n",
    "    Attributes:\n",
    "        clip_model (ClipModel): An object for data classification.\n",
    "        intents (List[str]): A list of categories for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize an ImageClassifier instance.\n",
    "        \"\"\"\n",
    "        self.clip_model = ClipModel()  # Initialize the ClipModel\n",
    "        self.intents = [\"payment-problem\", \"course-problem\", \"profile-problem\"] \n",
    "\n",
    "    def classify_intent(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Method for classifying intent.\n",
    "\n",
    "        Parameters:\n",
    "            image_path (str): The path to the image.\n",
    "\n",
    "        Returns:\n",
    "            str: The category of intent.\n",
    "        \"\"\"\n",
    "        image = ImageProcessor(image_path)\n",
    "        prepared_image = image.prepare_image()\n",
    "        \n",
    "        probs = self.clip_model.classify(prepared_image,self.intents)\n",
    "        max_index = probs.index(max(probs))\n",
    "        \n",
    "        # Return the corresponding intent\n",
    "        return self.intents[max_index]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T12:11:25.773287400Z",
     "start_time": "2024-10-31T12:11:25.764769400Z"
    }
   },
   "id": "d872204760213514"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "classifier = ImageClassifier()\n",
    "result = classifier.classify_intent(\"payment.png\") # 'payment-problem'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T12:11:29.183659900Z",
     "start_time": "2024-10-31T12:11:27.573036900Z"
    }
   },
   "id": "cf7ab831bda8dc4d"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'payment-problem'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T12:11:29.190464600Z",
     "start_time": "2024-10-31T12:11:29.183659900Z"
    }
   },
   "id": "335adab9fb4acd3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment-problem\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-31T11:56:33.918066400Z",
     "start_time": "2024-10-31T11:56:31.662067300Z"
    }
   },
   "id": "a66fec1516b5197e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

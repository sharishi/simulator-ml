{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Задача — создать Python-функцию, которая будет проверять входящие сообщения и выявлять скрытые в них ссылки для дальнейшего анализа."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "309603f5b34fc3c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Основные компоненты регулярных выражений:\n",
    "\n",
    "- Шаблоны: По сути, это набор правил, которым следует regex при поиске данных.\n",
    "- Метасимволы: Это специальные символы, которые имеют уникальное значение для модуля regex. К ним относятся такие символы, как {}[]()^$.*?|\\.\n",
    "- Квантификаторы: Они определяют, сколько раз должен повторяться тот или иной участок шаблона. К общим квантификаторам относятся * (0 или более раз), + (1 или более раз) и ? (0 или 1 раз).\n",
    "- Специальные последовательности: Это последовательности символов, которые имеют специальное значение, например, \\d означает любую цифру, или \\s любой пробельный символ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d48b3b1ebd8adc18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Борьба с вредоносными URL-адресами\n",
    "Спамеры часто пытаются скрыть свои намерения, и один из распространённых способов — маскировка URL-адресов.\n",
    "\n",
    "Вот несколько популярных приемов:\n",
    "\n",
    "Сокращатели URL-адресов: Спамеры часто используют такие сервисы, как Bitly, TinyURL, чтобы сократить свои URL-адреса и сделать их менее узнаваемыми.\n",
    "Использование Punycode: Punycode позволяет кодировать Unicode символы в ASCII, что часто используется спамерами для маскировки URL. Например, \"www.xn--80ak6aa92e.com\" на самом деле является \"www.аррӏе.com\", что может ввести в заблуждение пользователей, думающих, что они посещают \"www.apple.com\".\n",
    "Скрытие на виду: Некоторые спамеры вставляют URL прямо в текст без использования стандартного формата (например, \"example.com\" вместо \"http://www.example.com\")."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "186164acaa432ff1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Задание\n",
    "Ваша задача — написать функцию parse_urls. Эта функция должна принимать строку сообщения, обнаруживать в ней URL-адреса с помощью регулярных выражений, проверять доступность этих URL и, в конечном итоге, возвращать словарь. В этом словаре ключи — это уникальные URL (нормализованные так, чтобы в них не входили \"www\", \"http://\" или \"https://\"), а значения — это количество их упоминаний в строке сообщения.\n",
    "\n",
    "Доступность URL в данной задаче нужно проверить с помощью библиотеки requests таким образом:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd4e738793a64530"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='monster.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001874922C410>: Failed to resolve 'monster.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "{'wikipedia.org': 1, 'wired.com': 1, 'linkedin.com': 1, 'stackoverflow.com/questions/tagged/python': 1, 'kayak.com': 1, 'bbc.co.uk': 1, 'cnn.com': 1, 'nytimes.com': 1, 'codecademy.com': 1, 'forbes.com': 1, 'en.wikipedia.org/wiki/Special': 1}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "import requests\n",
    "def normalize_domains(domains):\n",
    "    normalized_domains = []\n",
    "    for domain in domains:\n",
    "        # Убираем пробелы, точки в конце и слэш, если он есть\n",
    "        clean_domain = domain.strip().rstrip('/').rstrip('.')\n",
    "        normalized_domains.append(clean_domain)\n",
    "    return normalized_domains\n",
    "        \n",
    "\n",
    "def parse_urls(message: str) -> Dict[str, int]:\n",
    "    \"\"\"Parse a list of URL strings and check their reachability.\"\"\"\n",
    "    # pattern = re.compile(r'(https?://)?(www\\.)?([a-zA-Z0-9-]+\\.(com|org|co\\.uk)(/[\\w\\-./?%&=]*)?)')\n",
    "    # pattern = re.compile(r'(https?://)?(www\\.)?([a-zA-Z0-9-]+\\.(com|org|co\\.uk))(/[\\w\\-./?%&=]*)?')\n",
    "    #?%&=\n",
    "    pattern = re.compile(r'(https?://)?(www\\.)?([a-zA-Z0-9.-]+\\.(com|org|co\\.uk)(/[\\w\\-./]*)?)')\n",
    "\n",
    "\n",
    "    matches = pattern.findall(message)\n",
    "    domains = [match[2] for match in matches]\n",
    "    new_domains = normalize_domains(domains)\n",
    "    \n",
    "\n",
    "    # Проверка доступности URL\n",
    "    reachable_urls = check_url_reachability(new_domains)\n",
    "\n",
    "    # Подсчет только доступных доменов\n",
    "    domain_count = Counter(reachable_urls)\n",
    "\n",
    "    return dict(domain_count)\n",
    "\n",
    "\n",
    "def check_url_reachability(urls: List[str]) -> List[str]:\n",
    "    \"\"\"Check the reachability of the list of URLs.\"\"\"\n",
    "    reachable_urls = []\n",
    "    for url in urls:\n",
    "        # Добавляем https:// перед проверкой\n",
    "        full_url = f\"https://{url}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.head(full_url, allow_redirects=True, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                reachable_urls.append(url)  # Сохраняем домен без протокола\n",
    "                continue\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            response = requests.head(full_url, allow_redirects=False, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                reachable_urls.append(url)  # Сохраняем домен без протокола\n",
    "        except requests.RequestException as e:\n",
    "            print(e)\n",
    "\n",
    "    return reachable_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    message = (\n",
    "        \"Quick visit to https://www.wikipedia.org/ for a wealth of knowledge.\"\n",
    "        \"Searching for recipes? Check http://allrecipes.com for inspiration.\"\n",
    "        \"For tech enthusiasts, https://www.wired.com is the place to be.\"\n",
    "        \"Connect with professionals at https://www.linkedin.com and expand your network.\"\n",
    "        \"Find your dream job at https://www.indeed.com or https://www.monster.com.\"\n",
    "        \"https://stackoverflow.com/questions/tagged/python has answers to many Python questions.\"\n",
    "        \"Discover amazing places on https://www.tripadvisor.com or book flights at https://www.kayak.com.\"\n",
    "        \"Enjoy shopping at https://www.amazon.com? Find similar deals at https://www.ebay.com.\"\n",
    "        \"Stay updated with news on https://www.bbc.co.uk or https://www.cnn.com.\"\n",
    "        \"Long string with multiple URLs: Check out the news at https://www.nytimes.com, grab some tech gadgets from https://www.bestbuy.com, find home improvement supplies at https://www.homedepot.com, look for health information on https://www.webmd.com, learn coding at https://www.codecademy.com, get financial advice from https://www.forbes.com, and explore https://en.wikipedia.org/wiki/Special:Random for a random Wikipedia article.\"\n",
    "    )\n",
    "    print(parse_urls(message))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a037c261c58fdb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def add_http_scheme(url: str) -> str:\n",
    "    if not url.startswith((\"http://\", \"https://\")):\n",
    "        return \"http://\" + url\n",
    "    return url\n",
    "\n",
    "\n",
    "def parse_urls(message: str) -> Dict[str, int]:\n",
    "    # Improved regex pattern to detect URLs without 'www' or 'http'\n",
    "    pattern = r'https?://[^\\s<>\",]+|www\\.[^\\s<>\",]+|(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}'\n",
    "\n",
    "    # Find all matches in the message\n",
    "    url_list = re.findall(pattern, message)\n",
    "\n",
    "    # Add http scheme if missing, and check if they are reachable\n",
    "    reachable_urls = []\n",
    "    print(url_list)\n",
    "    for url in url_list:\n",
    "        url_with_scheme = add_http_scheme(url)\n",
    "        try:\n",
    "            response = requests.head(\n",
    "                url_with_scheme,\n",
    "                timeout=5,\n",
    "            )\n",
    "            print(url_with_scheme, response.status_code)\n",
    "            url_norm = url_with_scheme.replace(\"http://\", \"\")\n",
    "            url_norm = url_norm.replace(\"https://\", \"\")\n",
    "            url_norm = url_norm.replace(\"www.\", \"\")\n",
    "\n",
    "            # remove punctuation at the end of url\n",
    "            if url_norm.endswith((\".\", \",\", \"!\", \"?\")):\n",
    "                url_norm = url_norm[:-1]\n",
    "\n",
    "            # remove / at the end of url\n",
    "            if url_norm.endswith(\"/\"):\n",
    "                url_norm = url_norm[:-1]\n",
    "\n",
    "            reachable_urls.append(url_norm)\n",
    "        except requests.RequestException as e:\n",
    "            # Handle exceptions for requests\n",
    "            print(e)\n",
    "\n",
    "    # Create a Counter object to count unique URLs\n",
    "    url_counts = Counter(reachable_urls)\n",
    "    unique_url_dict = dict(url_counts)\n",
    "    return unique_url_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    message = (\n",
    "        \"Check out this link www.example.com, example.com and\"\n",
    "        \" also https://www.xn--80ak6aa92e.com/\"\n",
    "        \" also www.xn--80ak6aa92e.com\"\n",
    "        \" also xn--80ak6aa92e.com\"\n",
    "        \" also apple.com\"\n",
    "        \" Don't miss this great opportunity!\"\n",
    "        \" www.google.com.\"\n",
    "        \" hello.ru\"\n",
    "    )\n",
    "    print(parse_urls(message))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee6d2a90769d1cf6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

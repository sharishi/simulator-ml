{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RECALL @ K\n",
    "Мы разрабатываем платформу Greenterest для хранения картинок, сгенерированных нейросетями.\n",
    "\n",
    "Мы строим поисковой сервис, который по запросу пользователя выдаёт 20+ релевантных изображений.\n",
    "\n",
    "Модель выдаёт меру релевантности картинок запросу.\n",
    "\n",
    "Чтобы оценить результаты модели, мы должны посмотреть на историю: какие были запросы в прошлом, какой SERP (Search Engine Result Page) был у старого алгоритма, на какие из картинок пользователь кликнул и т.д. В качестве таргета мы можем взять исторические реакции пользователей, а также проверить, как переранжировал бы картинки новый алгоритм. Будут ли в топ-20 более релевантные картинки, чем у старой системы, или нет?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b928916814bf3e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Мы реализуем 4 метрики, которые помогут нам оценить качество нашей модели во время оффлайн тестирования (т.е. до того, как раскатывать поиск на реальных пользователей):\n",
    "\n",
    "Recall @ K\n",
    "Precision @ K\n",
    "F1-Score @ K\n",
    "Specificity @ K"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca089ccb52b1f74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для примера, стандартный Recall в классификации сообщает, какая доля объектов положительного класса (в нашем случае, кликнутых картинок) включена моделью как положительный класс. Очевидно, для максимального Recall мы могли бы включить в положительный класс все картинки, но особого смысла от такой модели не будет. С другой стороны, тогда просядет Precision (сколько из рекомендованных картинок действительно были кликнуты). \n",
    "\n",
    "Чтобы избавиться от этой неопределённости, в ранжировании, поиске и рекомендациях используют метрики по типу Recall @ K, которые оперируют с предсказанным скором релевантности и топ-K от него сравнивают с положительным классом. Это довольно естественная метрика, потому что редко кто листает дальше первых 10-20 картинок. Соответственно, в топ-5 или топ-10 попадёт то, что пользователь точно увидит."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d1a517f715127c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Метрика Recall @ K (полнота при K) измеряет, сколько релевантных элементов из всех возможных попали в топ-K предложений (т.е. в первые K элементов). Она отражает, насколько хорошо ваша модель захватывает релевантные объекты среди K лучших результатов."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "390613b16bb84c3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision @ K (точность при K) измеряет долю правильных (релевантных) предложений среди первых K элементов, предложенных моделью. Эта метрика показывает, насколько точны топ-K предсказания: среди предложенных результатов, сколько из них действительно релевантны.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53cc9c0c47ccc52"
  },
  {
   "cell_type": "markdown",
   "source": [
    " F1-Score @ K:\n",
    "F1-Score — это гармоническое среднее между Precision и Recall, которое дает сбалансированную оценку точности и полноты. Используется, когда важно учитывать как пропущенные релевантные элементы (Recall), так и ложные срабатывания (Precision)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf0432f691ed114b"
  },
  {
   "cell_type": "markdown",
   "source": [
    " Specificity @ K:\n",
    "Specificity (специфичность) — это метрика, которая показывает долю правильно предсказанных нерелевантных элементов среди всех нерелевантных элементов. Она используется в задачах классификации, но в случае рекомендаций и ранжирования ее можно адаптировать для анализа топ-K предсказаний."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62fc5c68044ae7f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Пояснение:\n",
    "True Negatives (TN) — это нерелевантные элементы, которые не были предложены моделью (модель правильно их \"отвергла\").\n",
    "False Positives (FP) — это нерелевантные элементы, которые были предложены моделью (модель ошибочно их \"одобрила\")."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58afbfca5b855ebb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def recall_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Return the recall\"\"\"\n",
    "    sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    scores_k = sorted_indices[:k]\n",
    "    relevant_elements = [i for i, label in enumerate(labels) if label == 1]\n",
    "    relevant_at_k = set(scores_k) & set(relevant_elements)\n",
    "    return len(relevant_at_k) / len(relevant_elements) if len(relevant_elements) > 0 else 0.0\n",
    "\n",
    "\n",
    "def precision_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Return the precision\"\"\"\n",
    "    sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    scores_k = sorted_indices[:k]\n",
    "    relevant_elements = [i for i, label in enumerate(labels) if label == 1]\n",
    "    relevant_at_k = set(scores_k) & set(relevant_elements)\n",
    "    return len(relevant_at_k) / k if k > 0 else 0.0\n",
    "\n",
    "\n",
    "def specificity_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\" Specify at k\"\"\"\n",
    "    sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    top_k_indices = sorted_indices[:k]\n",
    "\n",
    "    # Определяем нерелевантные элементы (где label == 0)\n",
    "    non_relevant_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "\n",
    "    # Считаем True Negatives: нерелевантные элементы, которые не попали в топ-K\n",
    "    true_negatives_at_k = len(set(non_relevant_indices) - set(top_k_indices))\n",
    "\n",
    "    # Общее количество нерелевантных элементов\n",
    "    total_non_relevant = len(non_relevant_indices)\n",
    "\n",
    "    if total_non_relevant == 0:\n",
    "        return 0.0  # Во избежание деления на ноль\n",
    "\n",
    "    return true_negatives_at_k / total_non_relevant\n",
    "\n",
    "\n",
    "def f1_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Returns the f1 at k\"\"\"\n",
    "    p_k = precision_at_k(labels, scores, k)\n",
    "    r_k = recall_at_k(labels, scores, k)\n",
    "    return 2 * p_k * r_k / (p_k + r_k) if (p_k + r_k) != 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e67ba563b555266"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "34b35e12ef202278"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def recall_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Compute recall at k.\n",
    "\n",
    "    Args:\n",
    "        y_true: list of true labels\n",
    "        y_pred: list of predicted labels\n",
    "        k: number of top labels to consider\n",
    "\n",
    "    Returns:\n",
    "        Recall at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute all rates\n",
    "    positive_class = np.argsort(scores)[::-1][:k]\n",
    "    negative_class = np.argsort(scores)[::-1][k:]\n",
    "    tp_rate = np.sum([labels[i] == 1 for i in positive_class])\n",
    "    fn_rate = np.sum([labels[i] == 1 for i in negative_class])\n",
    "\n",
    "    # Compute recall at k\n",
    "    recall = tp_rate / (tp_rate + fn_rate)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Compute precision at k.\n",
    "\n",
    "    Args:\n",
    "        y_true: list of true labels\n",
    "        y_pred: list of predicted labels\n",
    "        k: number of top labels to consider\n",
    "\n",
    "    Returns:\n",
    "        Precision at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute all rates\n",
    "    positive_class = np.argsort(scores)[::-1][:k]\n",
    "    tp_rate = np.sum([labels[i] == 1 for i in positive_class])\n",
    "    fp_rate = np.sum([labels[i] == 0 for i in positive_class])\n",
    "\n",
    "    # Compute precision at k\n",
    "    precision = tp_rate / (tp_rate + fp_rate)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def specificity_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Compute specificity at k.\n",
    "\n",
    "    Args:\n",
    "        y_true: list of true labels\n",
    "        y_pred: list of predicted labels\n",
    "        k: number of top labels to consider\n",
    "\n",
    "    Returns:\n",
    "        Specificity at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute all rates\n",
    "    positive_class = np.argsort(scores)[::-1][:k]\n",
    "    negative_class = np.argsort(scores)[::-1][k:]\n",
    "\n",
    "    tn_rate = np.sum([labels[i] == 0 for i in negative_class])\n",
    "    fp_rate = np.sum([labels[i] == 0 for i in positive_class])\n",
    "\n",
    "    # Compute specificity at k\n",
    "    specificity = tn_rate / ((tn_rate + fp_rate) or 1e-16)\n",
    "\n",
    "    return specificity\n",
    "\n",
    "\n",
    "def f1_at_k(labels: List[int], scores: List[float], k=5) -> float:\n",
    "    \"\"\"Compute f1 score at k.\n",
    "\n",
    "    Args:\n",
    "        y_true: list of true labels\n",
    "        y_pred: list of predicted labels\n",
    "        k: number of top labels to consider\n",
    "\n",
    "    Returns:\n",
    "        F1 score at k\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute precision and recall at k\n",
    "    precision = precision_at_k(labels, scores, k)\n",
    "    recall = recall_at_k(labels, scores, k)\n",
    "\n",
    "    # Compute f1 score at k\n",
    "    f1_score = 2 * precision * recall / ((precision + recall) or 1e-16)\n",
    "\n",
    "    return f1_score\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-06T07:21:19.203931600Z",
     "start_time": "2024-10-06T07:21:19.139242200Z"
    }
   },
   "id": "cc329bc990aa8317"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c440c7ea2d6a5079"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

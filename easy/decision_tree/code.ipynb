{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "В этой задаче мы научимся строить решающие деревья с нуля. Это несложный, но распространённый алгоритм машинного обучения. Деревья применяются для решения задач регрессии, классификации, аплифт-моделирования и ранжирования.\n",
    "\n",
    "Решающие деревья – составной элемент в ансамблях моделей: случайный лес, градиентный бустинг.\n",
    "\n",
    "Решающие деревья реализованы во многих популярных библиотеках. Однако ничто не может быть лучшим знакомством с ними и с механизмом их работы, как возможность один раз написать решающее дерево самому. Понимание этих принципов поможет как в реальной работе, так и на собеседованиях – особенно в случаях, когда речь пойдет о принципах работы упомянутого градиентного бустинга."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f27fc5ccca1bfd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Что такое решающее дерево?\n",
    "Решающее дерево – это бинарное дерево, в узлах которого содержатся условия вида \"возраст < 30\" (для численных признаков) или \"пол = мужской\" (для категориальных), а в листьях предсказание модели (класс в случае классификации или численное значение в случае регрессии). "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2dde0a60b4e6c39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задача регрессии\n",
    "Мы будем предсказывать количество дней просрочки. Клиент подает заявку на кредит, указывает ряд данных о себе, а также сумму и срок кредита. Наша задача предсказать количество дней просрочки. Для большинства клиентов это значение равно 0 дней. Но для некоторых клиентов это значение больше нуля. Чем выше прогнозное значение \"дней просрочки\", тем более рискованно для банка одобрить заявку на кредит.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a77a8553b27607a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для обучения алгоритма у нас есть размеченный датасет с заявками на кредит. Целевая переменная — delay_days. Наша задача обучить модель (решающее дерево), которая для новых клиентов будет предсказывать количество дней просрочки, чтобы среднеквадратичная ошибка была минимальной."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdbbf3e6272462a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Дерево строится по данным обучающей выборки, на которой мы знаем значение признаков и целевую переменную для каждого объекта. Затем на новых данных с помощью построенного дерева выполняется предсказание целевой переменной, как мы это делали для примера с Андреем и Петром. Только вместо \"одобрить/ не одобрить\" мы будем предсказывать количество дней."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b663f4c06ffa5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```sql\n",
    "SELECT\n",
    "    age,\n",
    "    income,\n",
    "    dependents,\n",
    "    has_property,\n",
    "    has_car,\n",
    "    credit_score,\n",
    "    job_tenure,\n",
    "    has_education,\n",
    "    loan_amount,\n",
    "    dateDiff('day', loan_start, loan_deadline) AS loan_period,\n",
    "    CASE\n",
    "        WHEN loan_payed > loan_deadline\n",
    "        THEN dateDiff('day', loan_deadline, loan_payed)\n",
    "        ELSE 0\n",
    "    END AS delay_days\n",
    "FROM\n",
    "    default.loan_delay_days\n",
    "ORDER BY\n",
    "    id;\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c1c1c55c02fe872"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Информационный критерий\n",
    "Информационный критерий — это метод оценки качества разбиения данных на две или более группы в решающем дереве. Это условие, по которому мы выбираем оптимальное разбиение данных в каждом узле дерева. Хороший информационный критерий помогает выбрать такой вариант разбиения выборки на две подвыборки, чтобы ветвление дерева наиболее осмысленно разбивало данные."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7de2c4a39eb16a29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Предсказание в задаче регрессии выполняется по среднему значению целевой переменной всех объектов, попавших в лист на этапе обучения. В первом примере среднее значение для листа слева и среднее значение для листа справа будет ближе к истинным значениям. Во втором примере среднее значение будет дальше."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab5007ec37ebd69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Критерий разбиения\n",
    "Для задач регрессии в качестве критерия разбиения обычно используется MSE (Mean Squared Error). MSE – это средний квадрат ошибки (разницы) между фактическими значениями целевой переменной и предсказанными. Мы считаем MSE до разбиения и после. То разбиение, которое даст наибольшее снижение ошибки – оптимальное."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dac2fc8af84f89d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для выбора варианта разбиения оценивают средневзвешенное значение критерия в обоих выборках. Если средневзвешенное значение критерия в выборках слева и справа лучше, значит разбиение хорошее."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60c644eb4da82088"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание\n",
    "Реализуйте две функции:\n",
    "\n",
    "mse – принимает на вход вектор значений, возвращает значение mse.\n",
    "weighted_mse – принимает на вход два вектора значений (слева и справа) и возвращает средневзвешенную mse."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302451c48448e3cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "MSE – это ошибка между истинными значениями и предсказанными. Как мы разобрали выше, предсказание в задаче регрессии – это среднее целевой переменной для всех объектов узла. \n",
    "\n",
    "Поэтому в качестве предсказанного значения вычислите среднее значение выборки. И найдите MSE между истинными значениями выборки и предсказанным (средним)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bf7c0d413e5e149"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mse(y: np.ndarray) -> float:\n",
    "    \"\"\"Compute the mean squared error of a vector.\"\"\"\n",
    "    y_mean = np.mean(y)\n",
    "    return np.mean((y - y_mean) ** 2)\n",
    "\n",
    "\n",
    "def weighted_mse(y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "    \"\"\"Compute the weighted mean squared error of two vectors.\"\"\"\n",
    "    mse_left = mse(y_left)\n",
    "    mse_right = mse(y_right)\n",
    "    total_len = len(y_left)+len(y_right)\n",
    "    return (mse_left*len(y_left)+mse_right*len(y_right))/total_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Сплит по одному признаку\n",
    "Датасет подготовлен. Информационный критерий считать умеем. \n",
    "\n",
    "Теперь научимся определять, какой сплит (разбиение) будет оптимальным. На этом шаге мы делаем сплит только по одному признаку. Выбираем такое значение признака, чтобы средневзвешенный MSE был минимальынм."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eec3252961a051fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание\n",
    "Реализуйте функцию split. Функция принимает на вход выборку (матрицу признаков X и вектор целевой переменной y) и индекс признака. Возвращает значение порога с наилучшим разбиением.\n",
    "\n",
    "Например, если разбиение выполняется по полю age, в качестве аргумента feature будет передано значение 0. В качестве результата может быть значение 42. Что значит, что все объекты с age <= 42.0 попадают в левую выборку, остальные в правую выборку."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeb561fdd6c62252"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Перебираем все значения признака:\n",
    "\n",
    "    Разделяем данные по порог -> подвыборка слева, подвыборка справа\n",
    "        Считаем взвешенный критерий для подвыборок слева и справа\n",
    "Возвращаем значение порога, который дает минимальное значение взвешенного критерия\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71cf56b84adaf006"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def split(X: np.ndarray, y: np.ndarray, feature: int) -> float:\n",
    "    \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "    my_x = X[:, feature]\n",
    "    best_threshold = None\n",
    "    min_mse = float('inf')\n",
    "\n",
    "    for x in np.unique(my_x):\n",
    "        left_mask = my_x <= x\n",
    "        right_mask = my_x > x\n",
    "        \n",
    "        y_left = y[left_mask]\n",
    "        y_right = y[right_mask]\n",
    "        \n",
    "        mse_ = weighted_mse(y_left, y_right)\n",
    "        \n",
    "        if mse < min_mse:\n",
    "            min_mse = mse\n",
    "            best_threshold = x\n",
    "\n",
    "    return best_threshold"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T19:08:05.115693200Z",
     "start_time": "2024-10-08T19:08:04.733925600Z"
    }
   },
   "id": "3fa00d3f8af8b345"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Сплит по всем признакам\n",
    "Отлично, мы научились разбивать выборку на две подвыборки по одному признаку, выбирая значения признака таким образом, чтобы минимизировать средневзвешенный критерий MSE.\n",
    "\n",
    "Теперь сделаем то же самое, но для всех признаков. Ваша задача — найти наилучший сплит среди всех признаков."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8290a82f1564d07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание\n",
    "Реализуйте функцию best_split. Функция принимает на вход выборку (матрицу признаков X и вектор целевой переменной y). Возвращает индекс признака и значение порога с наилучшим разбиением."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34fa08b5ed6a0d74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def best_split(X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "    \"\"\"Find the best split for a node (one feature)\"\"\"\n",
    "    bests = {}\n",
    "    \n",
    "    for i in range(X.shape[1]): \n",
    "        bests[i] = split(X, y, i)\n",
    "        \n",
    "    best_threshold = min(bests.values())  \n",
    "    best_feature = min(bests, key=bests.get)  \n",
    "\n",
    "    return best_feature, best_threshold"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e4479486f40259f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь реализуем класс Node. Это вспомогательный класс. Каждый экземпляр класса описывает отдельный узел или лист в дереве. Из этих нод мы и будем строить дерево."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3708d1edb7ff2c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Давайте подумаем, какие атрибуты и каких типов необходимо добавить для класса Node:\n",
    "\n",
    "feature – индекс признака, по которому выборка, попавшая в ноду, разделяется на 2 дочерние ноды. Тип: int\n",
    "threshold – значение порога. Все элементы, у которых значение признака feature <= threshold, попадают в левую дочернюю ноду. Остальные в правую дочернюю ноду. Тип: float\n",
    "n_samples – количество объектов в ноде. Тип: int\n",
    "value – среднее значение целевой переменной среди всех объектов в ноде, округленное до целого. Тип: int\n",
    "mse – значение критерия MSE в ноде. Тип: float\n",
    "left – левая дочерняя нода. Тип: Node\n",
    "right – правая дочерняя нода. Тип: Node\n",
    "Для листьев также будем использовать класс Node. Только поля feature, threshold, left, right не будут заполнены."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21a2b1f8b4f8fbcf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задание\n",
    "Вам дана заготовка класса Node с тремя заполненными атрибутами. Завершите класс полностью, добавив все оставшиеся атрибуты.\n",
    "\n",
    "Обратите внимание, что класс Node мы реализуем как датакласс. Это более удобная и компактная запись. Если вам не требуется реализовывать дополнительную логику при инициализации экземпляра класса, то используйте датаклассы. Ваш код будет выглядет более читабельным. Подробнее о датаклассах можно почитать по ссылке.\n",
    "\n",
    "Для всех атрибутов укажите значение None в качестве дефолтного. Кажется, для числовых атрибутов это выглядит странным и можно использовать дефолтный 0. Но дефолтный 0 может совпасть с значением индекса признака, значением порога или mse. Поэтому более универсально для не заполненных значений использовать None. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5932127adfcf6eda"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A node in a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : int, optional (default=None)\n",
    "        The feature index used for splitting the node.\n",
    "    threshold : float, optional (default=None)\n",
    "        The threshold value at the node.\n",
    "    n_samples : int, optional (default=None)\n",
    "        The number of samples at the node.\n",
    "    value : int, optional (default=None)\n",
    "        The value of the node (i.e., the mean target value of the samples at the node).\n",
    "    mse : float, optional (default=None)\n",
    "        The mean squared error of the node (i.e., the impurity criterion).\n",
    "    left : Node, optional (default=None)\n",
    "        The left child node.\n",
    "    right : Node, optional (default=None)\n",
    "        The right child node.\n",
    "    \"\"\"\n",
    "       \n",
    "    feature: int = None\n",
    "    threshold: float = None\n",
    "    n_samples: int = None\n",
    "    value: int = None\n",
    "    mse: float = None\n",
    "    left: Node = None\n",
    "    right: Node = None\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T18:43:03.936936200Z",
     "start_time": "2024-10-09T18:43:03.930180600Z"
    }
   },
   "id": "20ef5878e1475861"
  },
  {
   "cell_type": "markdown",
   "source": [
    "DecisionTreeRegressor\n",
    "Подведем промежуточный итог, чему мы уже научились:\n",
    "\n",
    "1. Считать информационный критерий и взвешенный критерий, чтобы оценить качество сплита.\n",
    "2. Находить наилучшее разбиение выборки.\n",
    "3. Определили сущность нода и описали ее классом Node.\n",
    "# Пора переходить к построению дерева."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5a8c30c8f1ed2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Как строится дерево?\n",
    "Для построение дерева используется рекурсивный алгоритм. Корневая нода делится на две дочерние ноды. Каждая дочерняя нода в свою очередь делится еще на две дочерние ноды. Деление продолжается, пока не будет достигнут критерий остановки. В рекурсивных алгоритмах это еще называется базовый случай."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca87e738290db591"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разделить_ноду (Node):\n",
    "    Если достигли критерия остановки, то останавливаемся.\n",
    "    Ищем лучшее разбиение:\n",
    "        Если нашли лучшее разбиение, разбиваем Node на две дочерние Left и Right:\n",
    "            Разделить_ноду(Left)\n",
    "            Разделить_ноду(Right)\n",
    "        Иначе останавливаемся.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52bd9eb503e8aa98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Критерии остановки\n",
    "На практике используется много критериев остановки. Мы реализуем только два:\n",
    "\n",
    "- max_depth (максимальная глубина дерева) — если достигли максимальной глубины, нода дальше не делится и остается листом.\n",
    "- min_samples_split (минимальное число объектов в ноде для дальнейшего деления) — если объектов меньше, нода дальше не делится и остается листом."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f72fb41e10b471a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depth-wise подход\n",
    "В этой задаче мы используется подход построения дерева, который называется depth-wise (в глубину). Его идея в том, что каждая нода делится независимо до тех пор, пока не будет достигнут один из критериев остановки. Дерево очень быстро растет в глубину. Причем растет достаточно равномерно. Такой подход по умолчанию используется, например, при построении деревьев в популярной библиотеке XGBoost."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c51afbb4bf4cbe43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание\n",
    "Реализуйте класс DecisionTreeRegressor.\n",
    "\n",
    "Метод fit — это публичный метод класса (api класса). Он предназначен для использования внешним кодом, который создает экземпляры класса и манипулируют ими.\n",
    "\n",
    "Методы, названия которых начинаются с символа \"_\", называются внутренними методами. Они реализуют внутреннюю логику класса и не предназначены для использования снаружи.\n",
    "\n",
    "Перенесите код из реализованных ранее функций: mse, weighted_mse, best_split в одноименные внутренние методы. Перенести класс Node.\n",
    "\n",
    "Реализуйте внутренний метод _split_node"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea0ef92ca459cfea"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"Decision tree regressor.\"\"\"\n",
    "    max_depth: int = 5\n",
    "    min_samples_split: int = 2\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> DecisionTreeRegressor:\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._split_node(X, y)\n",
    "        return self\n",
    "\n",
    "    def _mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mse criterion for a given set of target values.\"\"\"\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _weighted_mse(self, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "        \"\"\"Compute the weighted mse criterion for a two given sets of target values\"\"\"\n",
    "        num = self._mse(y_left) * y_left.size + self._mse(y_right) * y_right.size\n",
    "        den = y_left.size + y_right.size\n",
    "        return num / den\n",
    "    \n",
    "    \n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Find the best split for a node.\"\"\"\n",
    "        node_size = y.size\n",
    "        n_features_ = X.shape[1]\n",
    "        if node_size < 2:\n",
    "            return None, None\n",
    "        node_mse = self._mse(y)\n",
    "        best_mse = node_mse\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(n_features_):\n",
    "            thresholds = np.unique(X[:, idx])\n",
    "            for thr in thresholds:\n",
    "                left = y[X[:, idx] <= thr]\n",
    "                right = y[X[:, idx] > thr]\n",
    "\n",
    "                if left.size == 0 or right.size == 0:\n",
    "                    continue\n",
    "\n",
    "                weihted_mse = self._weighted_mse(left, right)\n",
    "                if weihted_mse < best_mse:\n",
    "                    best_mse = weihted_mse\n",
    "                    best_idx = idx\n",
    "                    best_thr = thr\n",
    "        return best_idx, best_thr\n",
    "\n",
    "\n",
    "    def _split_node(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"Split a node and return the resulting left and right child nodes.\"\"\"\n",
    "        if depth == self.max_depth or y.size < 2:\n",
    "            return Node(value=int(np.mean(y)))  \n",
    "\n",
    "        # Преобразование X в DataFrame для использования .iloc\n",
    "        X_df = pd.DataFrame(X)\n",
    "    \n",
    "        best_idx, best_thr = self._best_split(X, y)\n",
    "        if best_thr is None:\n",
    "            return Node(value=int(np.mean(y)))  \n",
    "\n",
    "        left_idx = X_df.iloc[:, best_idx] <= best_thr  # Use .iloc for indexing\n",
    "        right_idx = X_df.iloc[:, best_idx] > best_thr   # Use .iloc for indexing\n",
    "    \n",
    "        left_child = self._split_node(X[left_idx.values], y[left_idx.values], depth + 1)\n",
    "        right_child = self._split_node(X[right_idx.values], y[right_idx.values], depth + 1)\n",
    "    \n",
    "        return Node(feature=best_idx, threshold=best_thr, left=left_child, right=right_child)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T20:38:06.237874600Z",
     "start_time": "2024-10-09T20:38:06.217841200Z"
    }
   },
   "id": "999df55fdcfafbd5"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b176b956d00754"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Исходные данные\n",
    "data = {\n",
    "    'age': [76, 69, 19, 31, 18, 51, 67, 27, 61, 61],\n",
    "    'income': [32181, 52789, 70535, 85271, 19974, 74128, 34922, 54154, 76998, 41396],\n",
    "    'dependents': [3, 8, 1, 1, 2, 3, 10, 1, 8, 5],\n",
    "    'has_property': [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'has_car': [1, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "    'credit_score': [814, 501, 325, 525, 618, 551, 657, 740, 869, 636],\n",
    "    'job_tenure': [28, 28, 26, 29, 34, 14, 19, 38, 35, 6],\n",
    "    'has_education': [1, 1, 1, 1, 1, 0, 0, 1, 1, 0],\n",
    "    'loan_amount': [142434, 120887, 188766, 406792, 155240, 257944, 207532, 229763, 147957, 483916],\n",
    "    'loan_term': [1770, 1590, 810, 330, 1560, 420, 240, 660, 90, 120],\n",
    "    'delay_days': [0, 7, 0, 0, 43, 0, 3, 0, 0, 12]\n",
    "}\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Признаки (X) и целевая переменная (y)\n",
    "X_train = df.drop(columns=['delay_days'])  # Убираем колонку 'delay_days'\n",
    "y_train = df['delay_days']  # Целевая переменная - 'delay_days'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T20:38:07.823576700Z",
     "start_time": "2024-10-09T20:38:07.772107Z"
    }
   },
   "id": "2d6616b387488d4"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\simulator\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:173\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mInvalidIndexError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m tree \u001B[38;5;241m=\u001B[39m DecisionTreeRegressor()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[34], line 17\u001B[0m, in \u001B[0;36mDecisionTreeRegressor.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_ \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_node\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "Cell \u001B[1;32mIn[34], line 65\u001B[0m, in \u001B[0;36mDecisionTreeRegressor._split_node\u001B[1;34m(self, X, y, depth)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m# Преобразование X в DataFrame для использования .iloc\u001B[39;00m\n\u001B[0;32m     63\u001B[0m X_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(X)\n\u001B[1;32m---> 65\u001B[0m best_idx, best_thr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_best_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m best_thr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Node(value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m(np\u001B[38;5;241m.\u001B[39mmean(y)))  \n",
      "Cell \u001B[1;32mIn[34], line 41\u001B[0m, in \u001B[0;36mDecisionTreeRegressor._best_split\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m     39\u001B[0m best_idx, best_thr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_features_):\n\u001B[1;32m---> 41\u001B[0m     thresholds \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(\u001B[43mX\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m thr \u001B[38;5;129;01min\u001B[39;00m thresholds:\n\u001B[0;32m     43\u001B[0m         left \u001B[38;5;241m=\u001B[39m y[X[:, idx] \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m thr]\n",
      "File \u001B[1;32m~\\PycharmProjects\\simulator\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   4101\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 4102\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4103\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   4104\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\PycharmProjects\\simulator\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3817\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m-> 3817\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_indexing_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3818\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\simulator\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6059\u001B[0m, in \u001B[0;36mIndex._check_indexing_error\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   6055\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_indexing_error\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):\n\u001B[0;32m   6056\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_scalar(key):\n\u001B[0;32m   6057\u001B[0m         \u001B[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001B[39;00m\n\u001B[0;32m   6058\u001B[0m         \u001B[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001B[39;00m\n\u001B[1;32m-> 6059\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n",
      "\u001B[1;31mInvalidIndexError\u001B[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T20:38:09.130318300Z",
     "start_time": "2024-10-09T20:38:09.074425700Z"
    }
   },
   "id": "ca5f4da7ad27fc11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Node for the decision tree.\"\"\"\n",
    "    feature: int = None\n",
    "    threshold: float = None\n",
    "    left: Node = None\n",
    "    right: Node = None\n",
    "    value: int = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"Decision tree regressor.\"\"\"\n",
    "    max_depth: int = 5\n",
    "    min_samples_split: int = 2\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series) -> DecisionTreeRegressor:\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._split_node(X, y)\n",
    "        return self\n",
    "\n",
    "    def _mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mse criterion for a given set of target values.\"\"\"\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _weighted_mse(self, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "        \"\"\"Compute the weighted mse criterion for two given sets of target values.\"\"\"\n",
    "        num = self._mse(y_left) * y_left.size + self._mse(y_right) * y_right.size\n",
    "        den = y_left.size + y_right.size\n",
    "        return num / den\n",
    "    \n",
    "    def _best_split(self, X: pd.DataFrame, y: pd.Series) -> tuple[int, float]:\n",
    "        \"\"\"Find the best split for a node.\"\"\"\n",
    "        node_size = y.size\n",
    "        n_features_ = X.shape[1]\n",
    "        if node_size < 2:\n",
    "            return None, None\n",
    "        node_mse = self._mse(y)\n",
    "        best_mse = node_mse\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(n_features_):\n",
    "            thresholds = np.unique(X.iloc[:, idx])  # Use .iloc for indexing\n",
    "            for thr in thresholds:\n",
    "                left = y[X.iloc[:, idx] <= thr]  # Use .iloc for indexing\n",
    "                right = y[X.iloc[:, idx] > thr]   # Use .iloc for indexing\n",
    "\n",
    "                if left.size == 0 or right.size == 0:\n",
    "                    continue\n",
    "\n",
    "                weighted_mse = self._weighted_mse(left.to_numpy(), right.to_numpy())\n",
    "                if weighted_mse < best_mse:\n",
    "                    best_mse = weighted_mse\n",
    "                    best_idx = idx\n",
    "                    best_thr = thr\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _split_node(self, X: pd.DataFrame, y: pd.Series, depth: int = 0) -> Node:\n",
    "        \"\"\"Split a node and return the resulting left and right child nodes.\"\"\"\n",
    "        if depth == self.max_depth or y.size < 2:\n",
    "            return Node(value=int(np.mean(y)))  \n",
    "\n",
    "        best_idx, best_thr = self._best_split(X, y)\n",
    "        if best_thr is None:\n",
    "            return Node(value=int(np.mean(y)))  \n",
    "\n",
    "        left_idx = X.iloc[:, best_idx] <= best_thr  # Use .iloc for indexing\n",
    "        right_idx = X.iloc[:, best_idx] > best_thr   # Use .iloc for indexing\n",
    "        \n",
    "        left_child = self._split_node(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_child = self._split_node(X[right_idx], y[right_idx], depth + 1)\n",
    "        \n",
    "        return Node(feature=best_idx, threshold=best_thr, left=left_child, right=right_child)\n",
    "\n",
    "\n",
    "# Исходные данные\n",
    "data = {\n",
    "    'age': [76, 69, 19, 31, 18, 51, 67, 27, 61, 61],\n",
    "    'income': [32181, 52789, 70535, 85271, 19974, 74128, 34922, 54154, 76998, 41396],\n",
    "    'dependents': [3, 8, 1, 1, 2, 3, 10, 1, 8, 5],\n",
    "    'has_property': [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'has_car': [1, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "    'credit_score': [814, 501, 325, 525, 618, 551, 657, 740, 869, 636],\n",
    "    'job_tenure': [28, 28, 26, 29, 34, 14, 19, 38, 35, 6],\n",
    "    'has_education': [1, 1, 1, 1, 1, 0, 0, 1, 1, 0],\n",
    "    'loan_amount': [142434, 120887, 188766, 406792, 155240, 257944, 207532, 229763, 147957, 483916],\n",
    "    'loan_term': [1770, 1590, 810, 330, 1560, 420, 240, 660, 90, 120],\n",
    "    'delay_days': [0, 7, 0, 0, 43, 0, 3, 0, 0, 12]\n",
    "}\n",
    "\n",
    "# Создаем DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Признаки (X) и целевая переменная (y)\n",
    "X_train = df.drop(columns=['delay_days'])  # Убираем колонку 'delay_days'\n",
    "y_train = df['delay_days']  # Целевая переменная - 'delay_days'\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31f4484f98b34a92"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=5, min_samples_split=2)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"Node for the decision tree.\"\"\"\n",
    "    feature: int = None\n",
    "    threshold: float = None\n",
    "    left: Node = None\n",
    "    right: Node = None\n",
    "    value: int = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"Decision tree regressor.\"\"\"\n",
    "    max_depth: int\n",
    "    min_samples_split: int = 2\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> DecisionTreeRegressor:\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._split_node(X, y)\n",
    "        return self\n",
    "\n",
    "    def _mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mse criterion for a given set of target values.\"\"\"\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _weighted_mse(self, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "        \"\"\"Compute the weighted mse criterion for two given sets of target values.\"\"\"\n",
    "        num = self._mse(y_left) * y_left.size + self._mse(y_right) * y_right.size\n",
    "        den = y_left.size + y_right.size\n",
    "        return num / den\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Find the best split for a node.\"\"\"\n",
    "        node_size = y.size\n",
    "        n_features_ = X.shape[1]\n",
    "        if node_size < 2:\n",
    "            return None, None\n",
    "        node_mse = self._mse(y)\n",
    "        best_mse = node_mse\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(n_features_):\n",
    "            thresholds = np.unique(X[:, idx])  # Use numpy indexing\n",
    "            for thr in thresholds:\n",
    "                left = y[X[:, idx] <= thr]  # Use numpy indexing\n",
    "                right = y[X[:, idx] > thr]   # Use numpy indexing\n",
    "\n",
    "                if left.size == 0 or right.size == 0:\n",
    "                    continue\n",
    "\n",
    "                weighted_mse = self._weighted_mse(left, right)\n",
    "                if weighted_mse < best_mse:\n",
    "                    best_mse = weighted_mse\n",
    "                    best_idx = idx\n",
    "                    best_thr = thr\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _split_node(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"Split a node and return the resulting left and right child nodes.\"\"\"\n",
    "        if depth == self.max_depth or y.size < self.min_samples_split:\n",
    "            return Node(value=int(np.mean(y)))  \n",
    "\n",
    "        best_idx, best_thr = self._best_split(X, y)\n",
    "        if best_thr is None:\n",
    "            return Node(value=int(np.mean(y)))  \n",
    "\n",
    "        left_idx = X[:, best_idx] <= best_thr  # Use numpy indexing\n",
    "        right_idx = X[:, best_idx] > best_thr   # Use numpy indexing\n",
    "        \n",
    "        left_child = self._split_node(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_child = self._split_node(X[right_idx], y[right_idx], depth + 1)\n",
    "        \n",
    "        return Node(feature=best_idx, threshold=best_thr, left=left_child, right=right_child)\n",
    "\n",
    "\n",
    "# Исходные данные\n",
    "data = {\n",
    "    'age': [76, 69, 19, 31, 18, 51, 67, 27, 61, 61],\n",
    "    'income': [32181, 52789, 70535, 85271, 19974, 74128, 34922, 54154, 76998, 41396],\n",
    "    'dependents': [3, 8, 1, 1, 2, 3, 10, 1, 8, 5],\n",
    "    'has_property': [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "    'has_car': [1, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
    "    'credit_score': [814, 501, 325, 525, 618, 551, 657, 740, 869, 636],\n",
    "    'job_tenure': [28, 28, 26, 29, 34, 14, 19, 38, 35, 6],\n",
    "    'has_education': [1, 1, 1, 1, 1, 0, 0, 1, 1, 0],\n",
    "    'loan_amount': [142434, 120887, 188766, 406792, 155240, 257944, 207532, 229763, 147957, 483916],\n",
    "    'loan_term': [1770, 1590, 810, 330, 1560, 420, 240, 660, 90, 120],\n",
    "    'delay_days': [0, 7, 0, 0, 43, 0, 3, 0, 0, 12]\n",
    "}\n",
    "\n",
    "# Создаем numpy массивы\n",
    "X_train = np.array([\n",
    "    [76, 32181, 3, 0, 1, 814, 28, 1, 142434, 1770],\n",
    "    [69, 52789, 8, 1, 0, 501, 28, 1, 120887, 1590],\n",
    "    [19, 70535, 1, 0, 1, 325, 26, 1, 188766, 810],\n",
    "    [31, 85271, 1, 0, 1, 525, 29, 1, 406792, 330],\n",
    "    [18, 19974, 2, 0, 1, 618, 34, 1, 155240, 1560],\n",
    "    [51, 74128, 3, 0, 1, 551, 14, 0, 257944, 420],\n",
    "    [67, 34922, 10, 1, 1, 657, 19, 0, 207532, 240],\n",
    "    [27, 54154, 1, 0, 1, 740, 38, 1, 229763, 660],\n",
    "    [61, 76998, 8, 0, 0, 869, 35, 1, 147957, 90],\n",
    "    [61, 41396, 5, 0, 0, 636, 6, 0, 483916, 120]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 7, 0, 0, 43, 0, 3, 0, 0, 12])  # Целевая переменная - 'delay_days'\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "tree = DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T20:42:00.542515600Z",
     "start_time": "2024-10-09T20:42:00.420751400Z"
    }
   },
   "id": "5917182a2d98de10"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeRegressor(max_depth=5, min_samples_split=2)"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A node in a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature : int, optional (default=None)\n",
    "        The feature index used for splitting the node.\n",
    "    threshold : float, optional (default=None)\n",
    "        The threshold value at the node.\n",
    "    n_samples : int, optional (default=None)\n",
    "        The number of samples at the node.\n",
    "    value : int, optional (default=None)\n",
    "        The value of the node (i.e., the mean target value of the samples at the node).\n",
    "    mse : float, optional (default=None)\n",
    "        The mean squared error of the node (i.e., the impurity criterion).\n",
    "    left : Node, optional (default=None)\n",
    "        The left child node.\n",
    "    right : Node, optional (default=None)\n",
    "        The right child node.\n",
    "    \"\"\"\n",
    "\n",
    "    feature: int = None\n",
    "    threshold: float = None\n",
    "    n_samples: int = None\n",
    "    value: int = None\n",
    "    mse: float = None\n",
    "    left: Node = None\n",
    "    right: Node = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DecisionTreeRegressor:\n",
    "    \"\"\"Decision tree regressor.\"\"\"\n",
    "    max_depth: int\n",
    "    min_samples_split: int = 2\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> DecisionTreeRegressor:\n",
    "        \"\"\"Build a decision tree regressor from the training set (X, y).\"\"\"\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._split_node(X, y)\n",
    "        return self\n",
    "\n",
    "    def _mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute the mse criterion for a given set of target values.\"\"\"\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _weighted_mse(self, y_left: np.ndarray, y_right: np.ndarray) -> float:\n",
    "        \"\"\"Compute the weighted mse criterion for two given sets of target values.\"\"\"\n",
    "        if y_left.size == 0 or y_right.size == 0:\n",
    "            return float('inf')\n",
    "        num = self._mse(y_left) * y_left.size + self._mse(y_right) * y_right.size\n",
    "        den = y_left.size + y_right.size\n",
    "        return num / den\n",
    "\n",
    "    def _best_split(self, X: np.ndarray, y: np.ndarray) -> tuple[int, float]:\n",
    "        \"\"\"Find the best split for a node.\"\"\"\n",
    "        node_size = y.size\n",
    "        if node_size < self.min_samples_split:\n",
    "            return None, None\n",
    "        node_mse = self._mse(y)\n",
    "        best_mse = node_mse\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(self.n_features_):\n",
    "            thresholds = np.unique(X[:, idx])\n",
    "            for thr in thresholds:\n",
    "                left = y[X[:, idx] <= thr]\n",
    "                right = y[X[:, idx] > thr]\n",
    "\n",
    "                if left.size == 0 or right.size == 0:\n",
    "                    continue\n",
    "\n",
    "                weighted_mse = self._weighted_mse(left, right)\n",
    "                if weighted_mse < best_mse:\n",
    "                    best_mse = weighted_mse\n",
    "                    best_idx = idx\n",
    "                    best_thr = thr\n",
    "\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _split_node(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\"Split a node and return the resulting left and right child nodes.\"\"\"\n",
    "        if depth == self.max_depth or y.size < self.min_samples_split:\n",
    "            return Node(value=int(round(np.mean(y))), mse=self._mse(y))\n",
    "\n",
    "        best_idx, best_thr = self._best_split(X, y)\n",
    "        if best_thr is None:\n",
    "            return None\n",
    "\n",
    "        left_idx = X[:, best_idx] <= best_thr\n",
    "        right_idx = X[:, best_idx] > best_thr\n",
    "\n",
    "        left_child = self._split_node(X[left_idx], y[left_idx], depth + 1)\n",
    "        right_child = self._split_node(X[right_idx], y[right_idx], depth + 1)\n",
    "\n",
    "        return Node(feature=best_idx, threshold=best_thr, left=left_child, right=right_child,\n",
    "                    value=int(round(np.mean(y))),\n",
    "                    mse=self._mse(y))\n",
    "\n",
    "\n",
    "# Создаем numpy массивы\n",
    "X_train = np.array([\n",
    "    [76, 32181, 3, 0, 1, 814, 28, 1, 142434, 1770],\n",
    "    [69, 52789, 8, 1, 0, 501, 28, 1, 120887, 1590],\n",
    "    [19, 70535, 1, 0, 1, 325, 26, 1, 188766, 810],\n",
    "    [31, 85271, 1, 0, 1, 525, 29, 1, 406792, 330],\n",
    "    [18, 19974, 2, 0, 1, 618, 34, 1, 155240, 1560],\n",
    "    [51, 74128, 3, 0, 1, 551, 14, 0, 257944, 420],\n",
    "    [67, 34922, 10, 1, 1, 657, 19, 0, 207532, 240],\n",
    "    [27, 54154, 1, 0, 1, 740, 38, 1, 229763, 660],\n",
    "    [61, 76998, 8, 0, 0, 869, 35, 1, 147957, 90],\n",
    "    [61, 41396, 5, 0, 0, 636, 6, 0, 483916, 120]\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 7, 0, 0, 43, 0, 3, 0, 0, 12])  # Целевая переменная - 'delay_days'\n",
    "\n",
    "# Создаем и обучаем модель\n",
    "tree = DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T07:27:55.809162800Z",
     "start_time": "2024-10-10T07:27:55.799646600Z"
    }
   },
   "id": "4e3cfa42b22ce435"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T20:52:34.821082400Z",
     "start_time": "2024-10-09T20:52:34.785562900Z"
    }
   },
   "id": "a1651bc0dc670924"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24951d8ca27b0740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

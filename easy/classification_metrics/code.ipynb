{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CLASSIFICATIONS METRICS\n",
    "В этой задаче мы рассмотрим основные метрики классификации, с которыми вы столкнетесь на практике и на собеседованиях."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11d3020e54457afb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задача классификации\n",
    "Задача классификации в машинном обучении заключается в прогнозировании категорий или классов отдельным наблюдениям на основе их характеристик. Отличительной чертой классификации является то, что целевая переменная является категориальной.\n",
    "\n",
    "Другими словами, мы предсказываем значение категориальной целевой переменной на основе одной или нескольких независимых переменных.\n",
    "\n",
    "Различают бинарную классификацию, когда целевая переменная может принимать только 2 значения (чаще всего их обозначают 0 и 1) или многоклассовая классификация, когда возможны несколько значений целевой переменной. В этой задаче мы будем говорить только о бинарной классификации."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dba92b75c98575a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. True Positive (TP)\n",
    "\n",
    "Случаи, когда модель правильно предсказывает положительный класс.\n",
    "\n",
    "Пример: Модель предсказывает наличие болезни, и пациент действительно болен.\n",
    "\n",
    "2. True Negative (TN)\n",
    "\n",
    "Случаи, когда модель правильно предсказывает отрицательный класс.\n",
    "\n",
    "Пример: Модель предсказывает отсутствие болезни, и пациент действительно здоров.\n",
    "\n",
    "3. False Positive (FP)\n",
    "\n",
    "Случаи, когда модель ошибочно предсказывает положительный класс.\n",
    "\n",
    "Пример: Модель предсказывает наличие болезни, но пациент здоров. Также известно как \"ложное срабатывание\" или \"Ошибка I рода\"\n",
    "\n",
    "4. False Negative (FN)\n",
    "\n",
    "Случаи, когда модель ошибочно предсказывает отрицательный класс.\n",
    "\n",
    "Пример: Модель предсказывает отсутствие болезни, но пациент болен. Также известно как \"пропущенное срабатывание\" или \"Ошибка II рода\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "512903576572112"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Порог вероятности (threshold)\n",
    "\n",
    "Модели классификации возвращают не бинарное значение (0 или 1), а вероятность, что объект относится к положительному классу (вещественное значение в диапазоне от 0 до 1). Чем сильнее модель уверена, что это отрицательный класс, тем ниже предсказанная вероятность. Чем больше уверена, что это положительный класс, тем выше вероятность. \n",
    "\n",
    "В процессе настройки модели подбирают порог вероятности, который разделяет объекты на положительные и отрицательные. Интуитивно кажется, что удобно использовать порог 0.5. Но дальше мы разберем, что в зависимости от решаемой задачи порог может быть выбран выше или ниже."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35dd7c6de9aca932"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Матрица ошибок (Confusion Matrix)\n",
    "Матрица ошибок — это таблица, используемая для описания производительности (качества) модели классификации на наборе тестовых данных, для которых известны истинные значения целевой переменной. Она позволяет легко визуализировать TP, TN, FP и FN."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a73c70cd71f9f7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Precision, Recall, F1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1581ee4320fd944f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Accuracy\n",
    "Accuracy (точность) - это доля правильно классифицированных наблюдений от общего числа наблюдений. Это самая базовая метрика для оценки моделей классификации.\n",
    "\n",
    "Accuracy интуитивно очень понятная метрика: \"В 85% случаев наша модель правильно отличает собачек от кошечек на фотографиях\".  Но пользоваться этой метрикой надо осторожно. Большой недостаток этой метрики, что она очень чувствительна к дисбалансу классов."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "272a0d8c0e0d50fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Precision\n",
    "Precision (точность) отражает, какая доля объектов, отнесенных моделью к положительному классу, действительно относится к этому классу."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dde0f3d0b028919"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall (полнота) показывает, какая доля объектов положительного класса была правильно идентифицирована моделью."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcd457d0cf60ea28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим для примера модель, предсказывающую есть ли золото в породе. Высокий pecision  будет означать, что большинство пород, которые модель классифицировала как содержащие золото действительно содержат его.\n",
    "\n",
    "Идеальная модель с точки зрения precision на столько \"боится сделать ошибку\", что ни одну из пород не классифицирует как содержащую золото. Потому что модель оценивают сколько действительно правильных ответов среди тех, которые она назвала как положительные.\n",
    "\n",
    "Recall наоборот показывает сколько истинно-положительных объектов нашла модель. Количество ложно-положительных ответов не имеет значения. Идеальная модель с точки зрения recall все объекты отметит как положительные, чтобы не пропустить ни одного.\n",
    "\n",
    "Поэтому модели классификации оценивают сразу по двум метрикам. Между precision и recall ищут tradeoff (компромисс). Улучшение одной метрики часто ведет к ухудшению другой. Если модель делает более строгие предсказания для классификации объекта как положительного, это может увеличить precision (меньше ложных срабатываний), но уменьшить recall (больше пропущенных положительных случаев). И наоборот.\n",
    "\n",
    "На практике tradeoff подбирают в зависимости от задачи. Подбор порога вероятности - это и есть tradeoff (выбор компромисса) между pecision и recall. Двигая порог вероятности ближе к нулю, мы делаем нашу систему решение менее строгим: больше объектов буду классифицированы как положительные. Смещение порога в сторону единицы, наоборот делает наше решение более строим.\n",
    "\n",
    "Если нам необходимо среди всех пользователей банковского приложения ловить только мошенников, tradeoff будет смещен в сторону precision. Потому что мы не хотим \"ловить за руку\" только тех пользователей, которые сильнее всех похожи на мошенников и не блокировать напрасно порядочных клиентов.\n",
    "Если нам важнее наоборот пропустить как можно меньше положительных примеров, то tradeoff будет смещен в сторону recall. Например, если мы диагностируем рак, то лучше много раз ошибиться и поставить ложно-положительный диагноз здоровым пациентам, чем пропустить действительно больных людей."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6c00ca6b5d383dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "F1-score (F1-мера) - это среднее гармоническое между Precision и Recall. Он используется, когда необходим баланс между Precision и Recall.  Появление этой метрики логичное желание найти какую-то одну метрику, которая будет учитывать сразу и precision и recall.\n",
    "\n",
    "Формула устроена таким образом, что если любая из метрик будет иметь низкое значение, то и F1 будет иметь низкое значение. Максимальное значение F1 возможно при разумном балансе между pecision и recall.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eff1e0d0e8c1840b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задание\n",
    "Многие библиотеки уже содержат эти метрики. Но в этом задании вам необходимо будет реализовать функции для расчета этих метрик самостоятельно.\n",
    "\n",
    "Напишите функции для расчета:\n",
    "\n",
    "confusion matrix\n",
    "accuracy\n",
    "precision\n",
    "recall\n",
    "f1-score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9958745fc8b56b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-20T14:48:58.704482Z",
     "start_time": "2024-10-20T14:48:58.596145100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, threshold: float) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Calculate confusion matrix.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    TP = np.sum((y_true == 1) & (y_pred_binary == 1))\n",
    "    TN = np.sum((y_true == 0) & (y_pred_binary == 0))\n",
    "    #ошибочно предсказываем положительный класс\n",
    "    FP = np.sum((y_true == 0) & (y_pred_binary == 1))# Ошибка I рода, ложное срабатывание\n",
    "    #ошибочно предсказываем отрицательный класс\n",
    "    FN = np.sum((y_true == 1) & (y_pred_binary == 0))## Ошибка II рода, пропущенное срабатывание\n",
    "\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def accuracy(TP: int, TN: int, FP: int, FN: int) -> float:\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "\n",
    "def precision(TP: int, FP: int) -> float:\n",
    "    \"\"\"Calculate precision.\"\"\"\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "\n",
    "def recall(TP: int, FN: int) -> float:\n",
    "    \"\"\"Calculate recall.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "\n",
    "def f1_score(precision: float, recall: float) -> float:\n",
    "    \"\"\"Calculate F1 score.\"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return 2 * (precision * recall)/(precision + recall)\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"Test function.\"\"\"\n",
    "    y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0, 0, 1])\n",
    "    y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2, 0.3, 0.6, 0.4, 0.5, 0.7])\n",
    "    threshold = 0.5\n",
    "    TP, TN, FP, FN = confusion_matrix(y_true, y_pred, threshold)\n",
    "\n",
    "    assert TP == 5\n",
    "    assert TN == 4\n",
    "    assert FP == 1\n",
    "    assert FN == 0\n",
    "\n",
    "    \n",
    "    assert np.allclose(accuracy(TP, TN, FP, FN), 0.9)\n",
    "\n",
    "    pr = precision(TP, FP)\n",
    "    re = recall(TP, FN)\n",
    "    assert np.allclose(pr, 0.8333333333333334)\n",
    "    assert np.allclose(re, 1)\n",
    "    assert np.allclose(f1_score(0.8333333333333334, 1), 0.9090909090909091)\n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def confusion_matrix(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, threshold: float\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Calculate confusion matrix.\"\"\"\n",
    "    TP = TN = FP = FN = 0\n",
    "\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp >= threshold:\n",
    "            TP += 1\n",
    "        elif yt == 0 and yp < threshold:\n",
    "            TN += 1\n",
    "        elif yt == 0 and yp >= threshold:\n",
    "            FP += 1\n",
    "        elif yt == 1 and yp < threshold:\n",
    "            FN += 1\n",
    "\n",
    "    return TP, TN, FP, FN\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-20T14:51:09.035254200Z",
     "start_time": "2024-10-20T14:51:09.009871300Z"
    }
   },
   "id": "c34540f27ef373c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Specificity и Sensitivity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59f9ae79451eccf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "На прошлом шаге мы разобрали, что кроме обучения модели  классификации также необходимо подобрать порог вероятности, какие объекты относить к положительному классу. И делается это как поиск компромисса между precision и recall.\n",
    "\n",
    "Для это строят PR-кривую (Precision-Recall  Curve). По этой кривой подбирают такое значение порога вероятности, при котором значение precision и recall будет оптимальным для решаемой задачи."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "115fda1c3b5d593c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Но у PR-кривой есть недостаток, который затрудняет этот процесс. Это не монотонная функция. То есть она может иметь локальные максимумы. А это делает подбор компромиса между precision и recall труднее."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b01d78defd7c2cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поэтому вместо precision и recall удобно использовать другие метрики.\n",
    "\n",
    "### Specificity (Специфичность)\n",
    "Измеряет способность модели правильно идентифицировать отрицательные случаи. То есть мы фокусируемся не на положительных классах, а наоборот на отрицательных."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a86d62f770e3dff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sensitivity (Чувствительность)\n",
    "Отражает способность модели правильно идентифицировать положительные случаи. Это синоним уже известной нам метрики recall."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1abe82a902db1fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Здесь также строят кривую, которая называется Sensitivity-Specificity Curve. В отличие от PR-кривой, здесь мы имеем дело с монотонной функцией. А значит подбор порога выглядит удобнее."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2acba517adc4e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Задание\n",
    "Напишите функцию для расчета sensitivity. Функцию confusion_matrix возьмите из предыдущего шага."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fbd6a582fb6175b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    " from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def confusion_matrix(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, threshold: float\n",
    ") -> Tuple[int, int, int, int]:\n",
    "    \"\"\"Calculate confusion matrix.\"\"\"\n",
    "    TP = TN = FP = FN = 0\n",
    "\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp >= threshold:\n",
    "            TP += 1\n",
    "        elif yt == 0 and yp < threshold:\n",
    "            TN += 1\n",
    "        elif yt == 0 and yp >= threshold:\n",
    "            FP += 1\n",
    "        elif yt == 1 and yp < threshold:\n",
    "            FN += 1\n",
    "\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def specificity(TN: int, FP: int) -> float:\n",
    "    \"\"\"Calculate specificity.\"\"\"\n",
    "    return TN/(TN+FP)\n",
    "\n",
    "def sensitivity(TP: int, FN: int) -> float:\n",
    "    \"\"\"Calculate sensitivity.\"\"\"\n",
    "    return TP/(TP+FN)\n",
    "\n",
    "def test():\n",
    "    \"\"\"Test function.\"\"\"\n",
    "    y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0, 0, 1])\n",
    "    y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2, 0.3, 0.6, 0.4, 0.5, 0.7])\n",
    "    threshold = 0.5\n",
    "    TP, TN, FP, FN = confusion_matrix(y_true, y_pred, threshold)\n",
    "\n",
    "    assert TP == 5\n",
    "    assert TN == 4\n",
    "    assert FP == 1\n",
    "    assert FN == 0\n",
    "\n",
    "    assert np.allclose(specificity(TN, FP), 0.8)\n",
    "    print(sensitivity(TP, FN))\n",
    "    print(\"All tests passed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-20T15:05:30.217805Z",
     "start_time": "2024-10-20T15:05:30.142278600Z"
    }
   },
   "id": "39c3744fe2ac06c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROC AUC\n",
    "Наконец рассмотрим одну из более популярных метрик для оценки качества моделей классификации.\n",
    "\n",
    "ROC-AUC (Receiver Operating Characteristic - Area Under the Curve) является важной метрикой, используемой для оценки качества моделей классификации. Для понимания этой метрики важно разобраться в нескольких ключевых аспектах.\n",
    "\n",
    "Важно разделять два понятия, которые часто путают начинающие специалисты:\n",
    "\n",
    "ROC-кривая - графическое представление качества модели\n",
    "ROC-AUC — площадь под ROC-кривой, численная метрика\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff2e9dade0674298"
  },
  {
   "cell_type": "markdown",
   "source": [
    "ROC-кривая\n",
    "Это графическое представление способности модели классификации различать два класса.\n",
    "\n",
    "Как строится?\n",
    "\n",
    "Ось X кривой представляет ложно-положительную долю (False Positive Rate, FPR), а ось Y — истинно-положительную долю (True Positive Rate, TPR). Каждая точка на кривой соответствует определенному порогу вероятности, при котором рассчитываются значения TPR и FPR.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "703d8d511e9cf39c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Интерпретация ROC-кривой:\n",
    "\n",
    "Высокий TPR при низком FPR указывает на высокое качество классификации.\n",
    "Кривая, близкая к левому верхнему углу, свидетельствует о лучшей классификационной способности модели.\n",
    "Диагональная линия соответствует случайному угадыванию.\n",
    "Если кривая лежит ниже диагональной линии, скорее всего перепутаны метки положительного и отрицательного классов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c650421e3dfb36d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "ROC-AUC\n",
    "Это площадь под ROC-кривой, численная мера, позволяющая оценить качество классификации.\n",
    "\n",
    "Как считается?\n",
    "\n",
    "AUC рассчитывается как отношение площади под ROC-кривой к общей площади графика. Она варьируется от 0 до 1, где 1 указывает на идеальную классификацию, а 0.5 — на отсутствие классификационной способности.\n",
    "\n",
    "В общем случае нет простой формулы расчета ROC AUC, аналогичной тем, которые используются для TPR или FPR, поскольку AUC представляет собой площадь под ROC-кривой. Эта площадь обычно вычисляется численно.\n",
    "\n",
    "Мы не будем писать эту метрику \"вручную\", и сразу воспользуемся готовыми функциями в Scikit-learn.\n",
    "\n",
    "Интерпретация:\n",
    "\n",
    "Большее значение ROC AUC указывает на лучшую способность модели различать классы.\n",
    "ROC AUC удобна для сравнения различных моделей, так как предоставляет единую численную оценку.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad152c6dddf196c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Преимущества ROC-кривой и ROC-AUC\n",
    "Универсальность: ROC AUC хорошо работает даже при дисбалансе классов.\n",
    "Наглядность: ROC-кривая позволяет визуализировать качество классификации на различных порогах одной кривой.\n",
    "Сравнительный анализ: Позволяет сравнивать разные модели по одной метрике, упрощая выбор наилучшей.\n",
    "Вы часто будете в своей практике сталкиваться с ROC-кривой и ROC AUC.\n",
    "\n",
    "Теперь давайте немного попрактикуемся!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1bf8382d52715f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание\n",
    "В этом задании вам предстоит обучить модель классификации и посчитать метрику ROC AUC\n",
    "\n",
    "Воспользуйтесь функцией roc_auc_score из библиотеки Scikit-learn.\n",
    "\n",
    "В качестве алгоритма рекомендуем выбрать логистическую регрессию из библиотеки Scikit-learn. Но это не обязательно, можно выбрать и любой другой алгоритм классификации."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eb3b37c5fb925d4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8653381642512078\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000, n_features=15, n_informative=10, random_state=42\n",
    "    )\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def solution(data: Tuple[np.ndarray, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Function to train a logistic regression model and calculate metrics.\n",
    "\n",
    "    Parameters:\n",
    "        data (tuple): Tuple with X and y.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with metrics.\n",
    "\n",
    "    Examples:\n",
    "        >>> solution()\n",
    "        {\n",
    "            'y_pred': array([0, 1, 1, 0]),\n",
    "            'y_test': array([0, 1, 1, 0]),\n",
    "            'roc_auc': 0.99\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    X, y = data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_test\": y_test,\n",
    "        \"roc_auc\": roc_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = prepare_data()\n",
    "    result = solution(data)\n",
    "    print(result[\"roc_auc\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-21T11:21:07.041079100Z",
     "start_time": "2024-10-21T11:21:05.396719600Z"
    }
   },
   "id": "dc589d98150a990c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "43036d55b1be01d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

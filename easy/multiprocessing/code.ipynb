{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Большие объемы данных часто являются возникновением проблем у специалистов по машинному обучению.\n",
    "\n",
    "В машинном обучение специалисты часто сталкиваются с большими объемами данных из-за чего алгоритмы могут работать достаточно долго. Давайте посмотрим на примере алгоритма Random Forest, как применение параллельных вычислений ускоряет время выполнения алгоритма.\n",
    "\n",
    "\n",
    "Для начала давайте разберемся, можем ли мы вообще применять параллельные вычисления в алгоритме Random Forest?\n",
    "\n",
    "Напомню, что алгоритм случайного леса строит множество деревьев на подмножествах из исходных данных и усредняет их результаты. Почему бы не строить деревья параллельно?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fd82df293654e03"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-01T20:20:36.093628900Z",
     "start_time": "2024-10-01T20:20:29.158425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_jobs: 1 | time fitting: 1.7408\n",
      "n_jobs: 2 | time fitting: 0.8706\n",
      "n_jobs: 3 | time fitting: 0.5813\n",
      "n_jobs: 4 | time fitting: 0.4711\n",
      "n_jobs: 5 | time fitting: 0.4176\n",
      "n_jobs: 6 | time fitting: 0.3516\n",
      "n_jobs: 7 | time fitting: 0.2986\n",
      "n_jobs: 8 | time fitting: 0.2756\n",
      "n_jobs: 9 | time fitting: 0.2752\n",
      "n_jobs: 10 | time fitting: 0.2581\n",
      "n_jobs: 11 | time fitting: 0.2451\n",
      "n_jobs: 12 | time fitting: 0.2346\n",
      "n_jobs: 13 | time fitting: 0.2252\n",
      "n_jobs: 14 | time fitting: 0.2382\n",
      "n_jobs: 15 | time fitting: 0.2255\n",
      "n_jobs: 16 | time fitting: 0.2105\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import multiprocessing\n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "#Generate a matrix 50000x1000 \n",
    "size = 1000\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "X = np.random.random((size, 1000)) \n",
    "y = np.random.randint(2, size=size) \n",
    "\n",
    "for n_jobs in range(1, num_cores+1): \n",
    "    start_time = time.time() \n",
    "    rfc = RandomForestClassifier(n_jobs=n_jobs)\n",
    "    rfc.fit(X, y)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f'n_jobs: {n_jobs} | time fitting: {end_time - start_time:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Объяснение, почему параллельные вычисления могут не работать на небольших размерах\n",
    "Накладные расходы на параллелизм: Параллельные вычисления требуют дополнительных ресурсов для управления потоками или процессами. Если объем данных небольшой, затраты на запуск и координацию параллельных задач могут превышать выгоду от их параллельной обработки.\n",
    "\n",
    "Сложность данных: Если массивы небольшие, время, необходимое для их обработки, может быть намного меньше времени, затрачиваемого на создание и управление потоками. В таких случаях использование единственного потока может оказаться более эффективным.\n",
    "\n",
    "Наличие GIL (Global Interpreter Lock): В Python существует GIL, который ограничивает одновременное выполнение потоков. Хотя это не столь критично для вычислительно интенсивных задач, как в случае с NumPy (где операции выполняются в C), он может влиять на производительность в некоторых сценариях.\n",
    "\n",
    "Эффективность кэширования: Параллельные вычисления могут нарушить кэширование данных, так как различные потоки могут обращаться к памяти. Для небольших массивов кэширование может быть более эффективным при использовании последовательного выполнения.\n",
    "\n",
    "Таким образом, параллельные вычисления более эффективны при работе с большими объемами данных, где их преимущества могут быть более заметными."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d9d11ad2a638b6"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e9387185e7fbd12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем написать простую функцию которая бездействует одну секунду и выполним ее 50 раз."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b6bd058e432aaa0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    3.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend MultiprocessingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    4.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:    5.2s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from time import sleep\n",
    "from typing import List\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def parallel(n_jobs=-1):\n",
    "    \"\"\"Parallel computing\"\"\"\n",
    "    result = Parallel(\n",
    "        n_jobs=n_jobs, backend=\"multiprocessing\", verbose=5 * n_jobs\n",
    "    )(delayed(sleep)(0.2) for _ in range(50))\n",
    "    return result\n",
    "\n",
    "\n",
    "print(parallel(n_jobs=1))\n",
    "print(parallel(n_jobs=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T20:24:02.616429Z",
     "start_time": "2024-10-01T20:23:47.285741100Z"
    }
   },
   "id": "51d148f8a7ec648e"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec0c47c7567351ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно ли ускорить чтение данных?\n",
    "Ваш коллега занимается обработкой естественного языка. Однако алгоритм в текущей реализации работает медленно. Вооружившись новыми знаниями о параллельных вычислениях, любезно согласились ему помочь.\n",
    "\n",
    "Вот его текущий код:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "846cd512dbb1427a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def clear_data(source_path: str, target_path: str):\n",
    "    \"\"\"Baseline process df\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_path : str\n",
    "        Path to load dataframe from\n",
    "\n",
    "    target_path : str\n",
    "        Path to save dataframe to\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(source_path)\n",
    "    data = data.copy().dropna().reset_index(drop=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    cleaned_text_list = []\n",
    "    for text in data[\"text\"]:\n",
    "        text = str(text)\n",
    "        text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
    "        text = re.sub(r\"@[^,\\s]+,?\", \"\", text)\n",
    "\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        transform_text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "        transform_text = re.sub(\" +\", \" \", transform_text)\n",
    "\n",
    "        text_tokens = word_tokenize(transform_text)\n",
    "\n",
    "        lemma_text = [\n",
    "            lemmatizer.lemmatize(word.lower()) for word in text_tokens\n",
    "        ]\n",
    "        cleaned_text = \" \".join(\n",
    "            [str(word) for word in lemma_text if word not in stop_words]\n",
    "        )\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "    data[\"cleaned_text\"] = cleaned_text_list\n",
    "    data.to_parquet(target_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T20:29:54.774834700Z",
     "start_time": "2024-10-01T20:29:54.363019600Z"
    }
   },
   "id": "dc3516fb51c59a23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задача\n",
    "Вам необходимо реализовать функцию clear_data, которая может параллельно обработать тексты в датасете."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe87a46cd575695d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def clear_data(source_path: str, target_path: str, n_jobs: int):\n",
    "    \"\"\"Parallel process dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_path : str\n",
    "        Path to load dataframe from\n",
    "\n",
    "    target_path : str\n",
    "        Path to save dataframe to\n",
    "\n",
    "    n_jobs : int\n",
    "        Count of job to process\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(source_path)\n",
    "    data = data.copy().dropna().reset_index(drop=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_text_list = []\n",
    "    \n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"Функция для очистки текста.\"\"\"\n",
    "        text = str(text)\n",
    "        text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
    "        text = re.sub(r\"@[^,\\s]+,?\", \"\", text)\n",
    "\n",
    "        stop_words = stopwords.words(\"english\")\n",
    "        transform_text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "        transform_text = re.sub(\" +\", \" \", transform_text)\n",
    "\n",
    "        text_tokens = word_tokenize(transform_text)\n",
    "\n",
    "        lemma_text = [\n",
    "            lemmatizer.lemmatize(word.lower()) for word in text_tokens\n",
    "        ]\n",
    "        cleaned_text = \" \".join(\n",
    "            [str(word) for word in lemma_text if word not in stop_words]\n",
    "        )\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "        return cleaned_text\n",
    "\n",
    "    result = Parallel(\n",
    "        n_jobs=n_jobs, backend=\"multiprocessing\", verbose=5 * n_jobs\n",
    "    )(delayed(clean_text)(text) for text in data[\"text\"])\n",
    "    \n",
    "    data[\"cleaned_text\"] = result\n",
    "    data.to_parquet(target_path)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcb95f975cd6c9f5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Функция для очистки текста.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # cleaned_text_list = []\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
    "    text = re.sub(r\"@[^,\\s]+,?\", \"\", text)\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    transform_text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    transform_text = re.sub(\" +\", \" \", transform_text)\n",
    "\n",
    "    text_tokens = word_tokenize(transform_text)\n",
    "\n",
    "    lemma_text = [\n",
    "        lemmatizer.lemmatize(word.lower()) for word in text_tokens\n",
    "    ]\n",
    "    cleaned_text = \" \".join(\n",
    "        [str(word) for word in lemma_text if word not in stop_words]\n",
    "    )\n",
    "    # cleaned_text_list.append(cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clear_data(source_path: str, target_path: str, n_jobs: int):\n",
    "    \"\"\"Parallel process dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_path : str\n",
    "        Path to load dataframe from\n",
    "\n",
    "    target_path : str\n",
    "        Path to save dataframe to\n",
    "\n",
    "    n_jobs : int\n",
    "        Count of job to process\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(source_path)\n",
    "    data = data.copy().dropna().reset_index(drop=True)\n",
    "\n",
    "    result = Parallel(\n",
    "        n_jobs=n_jobs, backend=\"multiprocessing\", verbose=5 * n_jobs\n",
    "    )(delayed(clean_text)(text) for text in data[\"text\"])\n",
    "\n",
    "    data[\"cleaned_text\"] = result\n",
    "    data.to_parquet(target_path)\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T20:50:33.576835300Z",
     "start_time": "2024-10-01T20:50:33.542321800Z"
    }
   },
   "id": "9d45ca7b1544b277"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def process_text(text: str):\n",
    "    \"\"\"Process text\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
    "    text = re.sub(r\"@[^,\\s]+,?\", \"\", text)\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    transform_text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "    transform_text = re.sub(\" +\", \" \", transform_text)\n",
    "\n",
    "    text_tokens = word_tokenize(transform_text)\n",
    "\n",
    "    lemma_text = [lemmatizer.lemmatize(word.lower()) for word in text_tokens]\n",
    "    cleaned_text = \" \".join(\n",
    "        [str(word) for word in lemma_text if word not in stop_words]\n",
    "    )\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clear_data(source_path: str, target_path: str, n_jobs: int):\n",
    "    \"\"\"Parallel process dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_path : str\n",
    "        Path to load dataframe from\n",
    "\n",
    "    target_path : str\n",
    "        Path to save dataframe to\n",
    "\n",
    "    n_jobs : int\n",
    "        Count of job to process\n",
    "    \"\"\"\n",
    "    data = pd.read_parquet(source_path)\n",
    "    data = data.copy().dropna().reset_index(drop=True)\n",
    "\n",
    "    count_stop_words = Parallel(n_jobs=n_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(process_text)(str(text)) for text in data[\"text\"]\n",
    "    )\n",
    "\n",
    "    data[\"cleaned_text\"] = count_stop_words\n",
    "    data.to_parquet(target_path)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b3bf316937ac274"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
